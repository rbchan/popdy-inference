\documentclass[color=usenames,dvipsnames]{beamer}
%\documentclass[color=usenames,dvipsnames,handout]{beamer}

\usepackage[roman]{../lectures}
%\usepackage[sans]{../lectures}


\hypersetup{pdfpagemode=UseNone,pdfstartview={FitV}}



% Load function to compile and open PDF
<<build-fun,include=FALSE,purl=FALSE>>=
source("../rnw2pdf.R")
@

% Compile and open PDF
<<buildit,include=FALSE,eval=FALSE>>=
rnw2pdf("lecture-Nmix-binomial-II")
rnw2pdf("lecture-Nmix-binomial-II", tangle=TRUE)
@ 


<<knitr-theme,include=FALSE,purl=FALSE>>=
##knit_theme$set("navajo-night")
knit_theme$set("edit-kwrite")
@


%% New command for inline code that isn't to be evaluated
\definecolor{inlinecolor}{rgb}{0.878, 0.918, 0.933}
\newcommand{\inr}[1]{\colorbox{inlinecolor}{\texttt{#1}}}




\begin{document}




\begin{frame}[plain]
  \LARGE
%  \maketitle
  \centering
  {\LARGE Lecture 7 -- Binomial $N$-mixture models: \\
    model selection and goodness-of-fit} \\  
  {\color{default} \rule{\textwidth}{0.1pt}}
  \vfill
  \large
  WILD(FISH) 8390 \\
  Estimation of Fish and Wildlife Population Parameters \\
  \vfill
  \large
  Richard Chandler \\
  University of Georgia \\
\end{frame}





\section{Model selection}



\begin{frame}[plain]
  \frametitle{Outline}
  \Large
  \only<1>{\tableofcontents}%[hideallsubsections]}
  \only<2 | handout:0>{\tableofcontents[currentsection]}%,hideallsubsections]}
\end{frame}





\begin{frame}
  \frametitle{Model selection}
  In scientific contexts, we want models that describe natural
  processes and allow us to evaluate hypotheses. \\
  \pause
  \vfill
  Models should be predictive, but they shouldn't be crafted with the
  sole goal of prediction in mind. \\
  \pause
  \vfill
  Scientists typically don't care about prediction if the model
  doesn't help us learn about the processes that gave rise to the
  data. \\  
  \pause
  \vfill
  Nonetheless, predictive performance is often the best way
  to compare models and avoid over-fitting. \\
  \pause
  \vfill
  Just make sure the models being compared were motivated by clear
  hypotheses.
\end{frame}




\begin{frame}
  \frametitle{Model selection}
  Prediction accuracy increases with model complexity up to a point,
  until over-fitting kicks in. \\
  \pause
  \vfill
  The best way to determine if a model is too simplistic or too
  complex is to compare predictions to new observations. \\
  \pause
  \vfill
  However, we rarely have the resources to
  collect additional data for the sake of evaluating predictive
  performance. \\ 
  % \pause
  % \vfill
  % Trouble is, people rarely collect new observations. \\
  \pause
  \vfill
  A cheaper (albeit less desirable) alternative is to use
  cross-validation: 
  \begin{itemize}
    \item Split data into K partitions
    \item Fit model to K-1 partitions
    \item Predict the holdout partition
  \end{itemize}
  \pause
  \vfill
  Information criteria like AIC and WAIC yield similar predictive
  rankings, but with less computation.    
\end{frame}



\subsection{Likelihood-based methods}



\begin{frame}
  \frametitle{Outline}
  \Large
  \tableofcontents[currentsection,currentsubsection]
\end{frame}



\bgroup
\let\oldfootnoterule\footnoterule
\def\footnoterule{\only<3->\oldfootnoterule}
\begin{frame}
  \frametitle{Model selection with AIC}
  \small
  Akaike's `An Information Criterion' approximates leave-one-out
  cross validation, and it's very easy to calculate:
  \[
%     \mathrm{AIC} = -2\times \mathrm{logLikelihood} + 2\times \mathrm{nParameters}
     \mathrm{AIC} = -2 \log(L) + 2P 
   \]
   where $L$ is the likelihood evaluated at the MLE and $P$ is
   the number of parameters.
  \pause
  \vfill
  For a binomial $N$-mixture model, the likelihood is computed by
  marginalizing (summing over all possible values of) $N_i$:
  \[
     L = \prod_{i=1}^M \sum_{k=\max(y_i)}^{K\approx \infty}
     \left\{\prod_{j=1}^J p(y_{ij}|N_i,p)\right\}p(N_i|\lambda)
  \]
  \pause
  where $p(y_{ij}|N_i,p)$ is the binomial probability
  density\footnote<3->{\scriptsize For discrete random variables, the  
    ``mass'' is often used instead of ``density''.} function 
  (pdf), and $p(N_i|\lambda)$ is the Poisson (or similar) pdf for
  local abundance. 
\end{frame}
\egroup



\bgroup
\let\oldfootnoterule\footnoterule
\def\footnoterule{\only<2->\oldfootnoterule}
\begin{frame}[fragile]
  \frametitle{Model selection in `unmarked'}
  \small
  Import the grouse data
  \vspace{-6pt}
<<grouse-in,size='footnotesize'>>=
library(unmarked)
grouse.data <- read.csv("grouse_data_Nmix.csv", row.names=1)
grouse.umf <- unmarkedFramePCount(
    y=grouse.data[,paste0("grouse",1:3)],
    siteCovs=grouse.data[,c("utmE","utmN","elevation")],
    obsCovs=list(temp=grouse.data[,paste0("Temperature.",1:3)]))
@
\pause
\vfill
Standardize the covariates\footnote<2->{\inr{scale} will only work if all the covariates are continuous}:
  \vspace{-6pt}
<<grouse-stand,size='footnotesize'>>=
site.covs.s <- scale(siteCovs(grouse.umf))
colnames(site.covs.s) <- paste0(colnames(site.covs.s), ".s")
siteCovs(grouse.umf) <- cbind(siteCovs(grouse.umf), site.covs.s)
obsCovs(grouse.umf) <- scale(obsCovs(grouse.umf))
@ 
\end{frame}
\egroup




\begin{frame}[fragile]
  \frametitle{Model selection in `unmarked'}
  \small
  Fit some models
  \vspace{-6pt}
<<grouse-mods,size='footnotesize',warning=FALSE,cache=TRUE>>=
fm1 <- pcount(~temp ~ elevation.s+utmE.s+utmN.s, grouse.umf, K=50)
fm2 <- pcount(~temp ~ elevation.s+utmN.s, grouse.umf, K=50)
fm3 <- pcount(~temp ~ elevation.s, grouse.umf, K=50)
fm4 <- pcount(~1 ~ elevation.s+utmN.s, grouse.umf, K=50)
fm5 <- pcount(~1 ~ elevation.s, grouse.umf, K=50)
fm6 <- pcount(~1 ~ 1, grouse.umf, K=50)
@
\pause
\vfill
Put models in a special type of list
  \vspace{-6pt}
<<grouse-fitlist,size='footnotesize',warning=FALSE>>=
grouse.models <- fitList('lam(elev+utmE+utmN)p(temp)'=fm1,
                         'lam(elev+utmN)p(temp)'=fm2,
                         'lam(elev)p(ptemp)'=fm3,
                         'lam(elev+utmN)p(.)'=fm4,
                         'lam(elev)p(.)'=fm5,
                         'lam(.)p(.)'=fm6)

@
\pause
Uh oh, missing values differ among models.
\end{frame}



\begin{frame}[fragile]
  \frametitle{Model selection in `unmarked'}
  \small
  Replace count data with \inr{NA} where associated covariates are missing:
  \vspace{-6pt}
<<addNA,size='footnotesize'>>=
na.sites <- apply(is.na(site.covs.s), 1, any)
grouse.counts <- getY(grouse.umf)
grouse.counts[na.sites,] <- NA
grouse.umf@y <- grouse.counts
@
  \pause
  \vfill
  Fit the models again
  \vspace{-6pt}
<<grouse-mods2,size='scriptsize',warning=FALSE,cache=TRUE>>=
fm1 <- pcount(~temp ~ elevation.s+utmE.s+utmN.s, grouse.umf, K=50)
fm2 <- pcount(~temp ~ elevation.s+utmN.s, grouse.umf, K=50)
fm3 <- pcount(~temp ~ elevation.s, grouse.umf, K=50)
fm4 <- pcount(~1 ~ elevation.s+utmN.s, grouse.umf, K=50)
fm5 <- pcount(~1 ~ elevation.s, grouse.umf, K=50)
fm6 <- pcount(~1 ~ 1, grouse.umf, K=50)
@
\pause
\vfill
Put models in a \inr{fitList}
  \vspace{-6pt}
<<grouse-fitlist2,size='scriptsize',warning=FALSE>>=
grouse.models <- fitList('lam(elev+utmE+utmN)p(temp)'=fm1,
                         'lam(elev+utmN)p(temp)'=fm2,
                         'lam(elev)p(ptemp)'=fm3,
                         'lam(elev+utmN)p(.)'=fm4,
                         'lam(elev)p(.)'=fm5,
                         'lam(.)p(.)'=fm6)

@
\end{frame}




\begin{frame}[fragile]
  \frametitle{Model selection in `unmarked'}
  \small
  Create AIC table
<<aic-table,size='scriptsize'>>=
modSel(grouse.models)
@
  \pause
  \vfill
  We could use the model with the lowest AIC for inference and
  prediction. \\ 
  \pause
  \vfill
  Or, we could model-average the predictions, which we'll cover later
  in the course. \\
  \pause
  \vfill
  For now, let's use the top model to predict grouse abundance across
  N Georgia.
\end{frame}




\begin{frame}
  \frametitle{Spatial prediction}
  When our covariates are available as raster layers, we can paint our
  predictions across the landscape. \\
  \pause
  \vfill
  This isn't the type of spatial modeling that you would learn about
  in a spatial statistics class because it doesn't account for spatial
  autocorrelation, except through the covariates. \\
  \pause
  \vfil
  However, it can be very useful nonetheless for modeling species
  distributions, and we will cover spatial autocorrelation later. 
\end{frame}



\begin{frame}
  \frametitle{Spatial prediction}

\end{frame}




\subsection{Bayesian methods}



\begin{frame}
  \frametitle{Outline}
  \Large
  \tableofcontents[currentsection,currentsubsection]
\end{frame}



\begin{frame}
  \frametitle{WAIC}
  
\end{frame}



\begin{frame}[fragile]
  \frametitle{Data, inits, and parameters}
  Put data in a named list
  \vspace{6pt}
<<bugs-data,size='scriptsize'>>=
jags.data <- list(
    y=grouse.counts,
    elevation=site.covs.s[,"elevation.s"],
    utmE=site.covs.s[,"utmE.s"],
    utmN=site.covs.s[,"utmN.s"],
    temp=as.matrix(grouse.data[,paste0("Temperature.",1:3)]),
    nSites=nrow(grouse.counts),
    nOccasions=ncol(grouse.counts))
jags.data$temp <- (jags.data$temp-mean(jags.data$temp, na.rm=TRUE))/
    sd(jags.data$temp, na.rm=TRUE) # standardize temperature
@
\pause
\vfill
  Initial values
  \vspace{-6pt}
<<bugs-inits,size='scriptsize'>>=
jags.inits <- function() {
    list(lambda.intercept=runif(1), alpha0=rnorm(1),
         N=rep(2, jags.data$nSites))
}
@ 
\pause
\vfill
  Parameters to monitor
  \vspace{-6pt}
<<bugs-pars,size='scriptsize'>>=
jags.pars <- c("beta0", "beta1", "beta2", "beta3",
               "alpha0", "alpha1", "totalAbundance",
               "ld.y.dot", "ld.ydot.N")
@ 
\end{frame}





\begin{frame}[fragile]
  \frametitle{The BUGS model}
  \small
  Notice \alert{\tt modswitch}, which we can use to include/exclude
  predictors. 
  \tiny
<<bugs,size='tiny',echo=FALSE>>=
writeLines(readLines("Nmix-model-grouse.jag"))
@
<<jagsUI,include=FALSE>>=
library(jagsUI)
@ 
\end{frame}





\begin{frame}[fragile]
  \frametitle{MCMC}
  Fit Model 1, corresponding to \inr{fm1}. 
<<bugs-mcmc,size='footnotesize',message=FALSE,cache=TRUE,results='hide'>>=
library(jagsUI)
jags.data1 <- jags.data
jags.data1$modswitch <- c(1,1,1,1) ## Include all covariates
jm1 <- jags.basic(data=jags.data1, inits=jags.inits,
                  parameters.to.save=jags.pars, 
                  model.file="Nmix-model-grouse.jag",
                  n.chains=3, n.adapt=100, n.burnin=0,
                  n.iter=2000, parallel=TRUE)
@
\pause
\vfill
  Fit Model 2
<<bugs-mcmc2,size='footnotesize',message=FALSE,cache=TRUE,results='hide'>>=
jags.data2 <- jags.data; jags.data2$modswitch <- c(1,0,1,1) 
jm2 <- jags.basic(data=jags.data2, inits=jags.inits,
                  parameters.to.save=jags.pars, 
                  model.file="Nmix-model-grouse.jag",
                  n.chains=3, n.adapt=100, n.burnin=0,
                  n.iter=2000, parallel=TRUE)
@ 
\end{frame}


\begin{frame}[fragile]
  \frametitle{MCMC}
  \small
  Model 3
  \vspace{-6pt}
<<bugs-mcmc3,size='tiny',message=FALSE,cache=TRUE,results='hide'>>=
jags.data3 <- jags.data; jags.data3$modswitch <- c(1,0,0,1)
jm3 <- jags.basic(data=jags.data3, inits=jags.inits, parameters.to.save=jags.pars,
                  model.file="Nmix-model-grouse.jag", n.chains=3, n.adapt=100, n.burnin=0,
                  n.iter=2000, parallel=TRUE)
@ 
  Model 4
  \vspace{-6pt}
<<bugs-mcmc4,size='tiny',message=FALSE,cache=TRUE,results='hide'>>=
jags.data4 <- jags.data; jags.data4$modswitch <- c(1,0,1,0)
jm4 <- jags.basic(data=jags.data4, inits=jags.inits, parameters.to.save=jags.pars,
                  model.file="Nmix-model-grouse.jag", n.chains=3, n.adapt=100, n.burnin=0,
                  n.iter=2000, parallel=TRUE)
@ 
  Model 5
  \vspace{-6pt}
<<bugs-mcmc5,size='tiny',message=FALSE,cache=TRUE,results='hide'>>=
jags.data5 <- jags.data; jags.data5$modswitch <- c(1,0,0,0)
jm5 <- jags.basic(data=jags.data5, inits=jags.inits, parameters.to.save=jags.pars,
                  model.file="Nmix-model-grouse.jag", n.chains=3, n.adapt=100, n.burnin=0,
                  n.iter=2000, parallel=TRUE)
@ 
  Model 6
  \vspace{-6pt}
<<bugs-mcmc6,size='tiny',message=FALSE,cache=TRUE,results='hide'>>=
jags.data6 <- jags.data; jags.data6$modswitch <- c(0,0,0,0)
jm6 <- jags.basic(data=jags.data6, inits=jags.inits, parameters.to.save=jags.pars,
                  model.file="Nmix-model-grouse.jag", n.chains=3, n.adapt=100, n.burnin=0,
                  n.iter=2000, parallel=TRUE)
@ 
\end{frame}


\begin{frame}[fragile]
  \frametitle{WAIC}
<<waic-fn-ld-op,size='small',include=FALSE>>=
waic <- function(x, focus=c("y", "yN")) {
    vars <- coda::varnames(x)
    if(focus[1]=="y") {
        ld.samples <- as.matrix(x[,grep("ld.y.dot", vars)])
    } else if(focus[1]=="yN") {
        ld.samples <- as.matrix(x[,grep("ld.ydot.N", vars)])
    } else stop("focus should be either 'y' or 'yN'")
    lppd <- sum(log(colMeans(exp(ld.samples))))
    penalty <- sum(apply(ld.samples, 2, var))
    return(-2*(lppd-penalty))
}
@   
<<waic-fn,size='footnotesize'>>=
waic <- function(x) {
    ## Parameter names
    vars <- coda::varnames(x)
    ## Extract log-density of y at each site for each post sample
    ld.samples <- as.matrix(x[,grep("ld.y.dot", vars)])
    ## Compute log-pointwise-predictive-density
    lppd <- sum(log(colMeans(exp(ld.samples))))
    ## Compute penalty
    penalty <- sum(apply(ld.samples, 2, var))
    ## Return WAIC
    return(-2*(lppd-penalty))
}
@   
\end{frame}


% \begin{frame}[fragile]
%   \frametitle{WAIC}
% <<waic1,size='scriptsize'>>=
% (waic1 <- waic(jm1, focus="yN"))
% @   
% <<waic2,size='scriptsize'>>=
% (waic2 <- waic(jm2, focus="yN"))
% @   
% <<waic6,size='scriptsize'>>=
% (waic6 <- waic(jm6, focus="yN"))
% @   
% \end{frame}



\begin{frame}[fragile]
  \frametitle{WAIC}
  \begin{columns}
    \begin{column}{0.5\textwidth}
<<waic1,size='scriptsize'>>=
(waic1 <- waic(jm1))
@   
<<waic2,size='scriptsize'>>=
(waic2 <- waic(jm2))
@   
<<waic3,size='scriptsize'>>=
(waic3 <- waic(jm3))
@   
    \end{column}
    \begin{column}{0.5\textwidth}
<<waic4,size='scriptsize'>>=
(waic4 <- waic(jm4))
@   
<<waic5,size='scriptsize'>>=
(waic5 <- waic(jm5))
@   
<<waic6,size='scriptsize'>>=
(waic6 <- waic(jm6))
@
    \end{column}
  \end{columns}
\end{frame}



\section{Goodness-of-fit}



\begin{frame}[plain]
  \frametitle{Outline}
  \Large
  \tableofcontents[currentsection]
\end{frame}



\begin{frame}
  \frametitle{Goodness-of-fit}
  \small
  Distributional assumptions determine the expected values
  \alert{and the expected variance} of the random variables, including
  the data. \\  
  \pause
  \vfill
  Overdispersion occurs when there is more variance in the data than
  expected by the model. \\
  \pause
  \vfill
  Goodness-of-fit method assess over and underdispersion. \\
  \pause
  \vfill
  If the model does not fit the data very well because of
  overdispersion, there are several remedial actions:
  \begin{itemize}
  \item<5-> Scientific approach
    \begin{itemize}
       \item Figure out why there is unexplained variation
       \item Perhaps there were unmeasured covariates or you need a
         better model for describing the processes
    \end{itemize}
  \item<6-> Statistical approach
    \begin{itemize}
      \item Soak up variation with random effects
      \item Use a different distribution (we'll start here)
    \end{itemize}
  \end{itemize}
\end{frame}






\bgroup
\let\oldfootnoterule\footnoterule
\def\footnoterule{\only<3->\oldfootnoterule}
\begin{frame}
  \frametitle{Changing the distribution for $N$}
  \small
  Standard Poisson-binomial $N$-mixture model (without covariates):
  \begin{gather*}
%    \mathrm{log}(\lambda_i) = \beta_0 + \beta_1 {\color{blue} x_{i1}} +
%    \beta_2 {\color{blue} x_{i2}} + \cdots \\
    N_i \sim \mathrm{Poisson}(\lambda) \\
%    \mathrm{logit}(p_{ij}) = \alpha_0 + \alpha_1 {\color{blue} x_{i1}}
%    + \alpha_2 {\color{Purple} w_{ij}} + \cdots \\
    y_{ij} \sim \mathrm{Binomial}(N_i, p)
  \end{gather*}
  \pause
%  \vfill
  We can replace the Poisson distribution with other distributions
  that allow for greater variance in $N$. \pause Two common examples
  are the negative 
  binomial:
  \begin{equation*}
    N_i \sim \mathrm{NegBin}(\lambda_i, \kappa)
  \end{equation*}
  where $\lambda_i$ is the expected value of $N$ and $\kappa$ is the
  dispersion parameter\footnote<3->{There are several other
    parameterizations of the negative binomial}.
  \pause
%  \vfill
  Another option is the zero-inflated Poisson:
  \begin{columns}
    \begin{column}{0.45\textwidth}
      \begin{gather*}
        N_i \sim \mathrm{Poisson}(\lambda_i z_i) \\
        z_i \sim \mathrm{Bern}(\psi) \\
      \end{gather*}
    \end{column}
    \begin{column}{0.1\textwidth}
%      \centering
%      Or \\
      \rule{0.1pt}{24pt} \\
    \end{column}
    \begin{column}{0.45\textwidth}
      \begin{gather*}
        \hspace{-72pt}
        N_i \sim \mathrm{ZIPoisson}(\lambda_i,\psi) \\
      \end{gather*}
    \end{column}
  \end{columns}
  where $\psi$ is the expected proportion of sites with excess zeros. 
\end{frame}
\egroup



\subsection{Likelihood-based methods}



\begin{frame}[fragile]
  \frametitle{Goodness-of-fit in `unmarked'}
  The parametric bootstrap can be used to assess model fit:
  \begin{enumerate}
    \item Simulate a dataset from the fitted model
    \item Fit the model to the new dataset
    \item Compute a fit statistic
    \item Repeat steps 1-3 several hundred/thousand times
    \item Compare the distribution of the \alert{expected} fit
      statistic to the fit statistic associated with
      the actual data. 
  \end{enumerate}
  The fit statistic associated with the actual data should be in an
  extreme quantile of the distribution of the expected values.
\end{frame}




\begin{frame}[fragile]
  \frametitle{Goodness-of-fit in `unmarked'}
<<parboot,size='small',warning=FALSE,cache=TRUE>>=
pb <- parboot(fm1, nsim=200, ncores=3)
plot(pb)
@   
\end{frame}




% \begin{frame}[fragile]
%   \frametitle{\normalsize Empirical Bayes -- Site-level abundance}
% <<ranef,size='scriptsize',out.width='80%',fig.align='center',fig.width=9>>=
% re <- ranef(fm)
% plot(re, layout=c(4,3), subset=site%in%1:12, xlim=c(-1, 11), lwd=5)
% @   
% \end{frame}





% \begin{frame}[fragile]
%   \frametitle{Total abundance (in surveyed region)}
% <<Ntotal,size='scriptsize',out.width='60%',fig.align='center'>>=
% N.total.post <- predict(re, func=sum, nsim=1000)
% hist(N.total.post, freq=FALSE, main="", xlab="N total", ylab="Probability")
% @   
% \end{frame}







\subsection{Bayesian methods}



\begin{frame}[plain]
  \frametitle{Outline}
  \Large
  \tableofcontents[currentsection,currentsubsection]
\end{frame}







\section{Assignment}




\begin{frame}[fragile]
  \frametitle{Assignment}
  % \small
  \footnotesize
  Create a self-contained R script or Rmarkdown file
  to do the following:
  \vfill
  \begin{enumerate}
%    \small
    \footnotesize
    \item a
    \item b
    \item c 
      \begin{itemize}
        \footnotesize
        \item i
        \item ii
      \end{itemize}
    \item d
  \end{enumerate}
  \vfill
  Upload your {\tt .R} or {\tt .Rmd} file to ELC before Monday. 
\end{frame}





\end{document}


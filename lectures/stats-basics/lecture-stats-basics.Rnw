\documentclass[color=usenames,dvipsnames]{beamer}
%\documentclass[color=usenames,dvipsnames,handout]{beamer}

\usepackage[roman]{../lectures}
%\usepackage[sans]{../lectures}


\hypersetup{pdfpagemode=UseNone,pdfstartview={FitV}}


\title{Lecture 2 -- Basic statistical models}
\author{Richard Chandler}


% Load function to compile and open PDF
<<build-fun,include=FALSE,purl=FALSE>>=
source("../rnw2pdf.R")
@

% Compile and open PDF
<<buildit,include=FALSE,eval=FALSE>>=
rnw2pdf("lecture-stats-basics")
rnw2pdf("lecture-stats-basics", tangle=TRUE)
@ 


<<knitr-theme,include=FALSE,purl=FALSE>>=
##knit_theme$set("navajo-night")
knit_theme$set("edit-kwrite")
@



%<<knitr-setup,include=FALSE,purl=FALSE>>=
%##opts_chunk$set(comment=NA)
%@


%% New command for inline code that isn't to be evaluated
\definecolor{inlinecolor}{rgb}{0.878, 0.918, 0.933}
\newcommand{\inr}[1]{\colorbox{inlinecolor}{\texttt{#1}}}




\begin{document}

% This would affect all code boxes. Not a good idea.
% \setlength\fboxsep{0pt}



\begin{frame}[plain]
  \LARGE
%  \maketitle
  \centering
  {\huge Lecture 2 -- Refresher of basic statistical models} \\
  {\color{default} \rule{\textwidth}{0.1pt}}
  \vfill
  \large
  WILD(FISH) 8390 \\
  Estimation of Fish and Wildlife Population Parameters \\
  \vfill
  \large
  Richard Chandler \\
  University of Georgia \\
\end{frame}




\section{Linear models}


\begin{frame}[plain]
  \frametitle{Today's Topics}
  \Large
  \only<1>{\tableofcontents}%[hideallsubsections]}
  \only<2 | handout:0>{\tableofcontents[currentsection]}%,hideallsubsections]}
\end{frame}




\begin{frame}
  \frametitle{Linear models}
%  \small
  All ANOVAs and fixed-effects regression models are linear models \\
  \vspace{12pt}
  You must understand linear models before you can apply more advanced
  models such as GLMs, GAMS, hierarchical models etc\dots  
  \vfill
  \centering
  \includegraphics[width=0.25\textwidth]{figure/Rencher_Schaal_book} \hspace{1cm}
  \includegraphics[width=0.25\textwidth]{figure/Hocking_book} \\
\end{frame}


\begin{frame}
  \frametitle{Simple linear regression}
  Simple linear regression is the most basic linear model.
  \[
    y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \qquad \qquad
    \varepsilon_i \sim \mathrm{Norm}(0, \sigma^2)
  \]
  \vspace{-6pt}
  \pause
  \begin{columns}[T]
    \begin{column}{0.5\textwidth}
      $y$ is variously called:
      \begin{itemize}
        \item response variable
        \item outcome variable
        \item dependent variable
      \end{itemize}
    \end{column}
    \pause
    \begin{column}{0.5\textwidth}
      It's worse for $x$
      \begin{itemize}
        \item predictor variable
        \item explanatory variable
        \item independent variable
        \item covariate
        \item exposure
      \end{itemize}
    \end{column}
  \end{columns}
  \pause
  \vfill
  We observe $x$ and $y$. \\
  \vfill
  We'd like to estimate $\beta_0$ and $\beta_1$, called the ``coefficients''. \\
  \pause
  \vfill
  The $\{\varepsilon_i\}$ are residuals. $\sigma^2$ is their variance.
\end{frame}



\begin{frame}[fragile]
  \frametitle{Probability distributions}
  Check out Heather Gaya's awesome Shiny App: \\
  \vfill
  \centering
  \href{ % Highlighting gets messed up if on 1 line
    https://insects.shinyapps.io/Probability_Dists/
  }{
    \large
    \color{blue}
    {https://insects.shinyapps.io/Probability\_Dists/} 
  } \\
  \vfill
  \includegraphics[width=0.7\textwidth]{figs/probDists} \\
\end{frame}






\begin{frame}[fragile]
  \frametitle{Simple linear regression example}
  Suppose your data look like this:
<<slr-data,size='scriptsize'>>=
x <- c(0.30, 0.91, 0.89, 0.24, 0.77, 0.56, 0.59, 0.92, 0.81, 0.59)
y <- c(0.08, 0.59, 0.18, 0.17, 0.42, 0.71, 0.49, 0.75, 0.46, 0.04)
@   
\begin{columns}
  \begin{column}{0.5\textwidth}
<<slr-viz,echo=FALSE>>=
plot(x,y)
abline(lm(y~x))
@     
  \end{column}
  \pause
  \begin{column}{0.5\textwidth}
    Classical inference is easy:
<<slr-fit,size='scriptsize'>>=
lm(y~x)
@ 
  \end{column}
\end{columns}
\end{frame}


\begin{frame}
  \frametitle{Bayesian inference with JAGS}
  We will use JAGS and the R packages `rjags' and `jagsUI'.
  JAGS can be downloaded here:  \\
  \centering
  \vfill
  \href{
    https://sourceforge.net/projects/mcmc-jags/files/}{
    \large
    \color{blue}
    https://sourceforge.net/projects/mcmc-jags/files/
  } \\
  \pause
  \vfill
  \flushleft
  The basic steps of Bayesian analysis with JAGS are:
  \begin{enumerate}%[<+->]
    \item Write a model in the BUGS language in a text file
    \item Put data in a named list
    \item Create a function to generate random initial values
    \item Specify the parameters you want to monitor
    \item Use MCMC to draw posterior samples
    \item Use posterior samples for inference and prediction
  \end{enumerate}
\end{frame}





\begin{frame}[fragile]
  \frametitle{Simple (Bayesian) linear regression}
  \small
  The model is in a text file named {\tt lm.jag} \\
<<lm-jag,size="scriptsize",comment="",echo=FALSE>>=
  writeLines(readLines("lm.jag"))
@
\pause
\vfill
Now, put the data in a list and pick some initial values
<<lm-jd>>=
jd.lm <- list(x=x, y=y, n=length(y))
ji.lm <- function() c(beta0=0, beta1=0, sigmaSq=1)
jp.lm <- c("beta0", "beta1", "sigmaSq")
@ 
\end{frame}


\begin{frame}[fragile]
  \frametitle{Simple (Bayesian) linear regression}
  Use MCMC to draw posterior samples:
<<lm-jags,size='scriptsize',results='hide',warning=FALSE,cache=FALSE>>=
library(jagsUI)  
js.lm <- jags.basic(data=jd.lm, inits=ji.lm, parameters.to.save=jp.lm,
                    model.file="lm.jag", n.chains=1, n.iter=1000)
@ 
\pause
\begin{columns}
  \begin{column}{0.55\textwidth}
    Summarize the posterior samples:
<<lm-jags-sum,size='scriptsize'>>=
round(summary(js.lm)$quant, 2)
@
\pause
  \end{column}
  \begin{column}{0.45\textwidth}
<<lm-jags-viz,echo=FALSE>>=
plot(js.lm)
@     
  \end{column}
\end{columns}
\end{frame}




\begin{frame}
  \frametitle{Linear models}
    A more general depiction of linear models:

\[
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \varepsilon_i
\]

where the $\beta$'s are coefficients, and the $x$ values are predictor
variables (or dummy variables for categorical predictors). %\pause
The residuals are assumed to be normally distributed:

\[
  \varepsilon_i \sim \mathrm{Norm}(0, \sigma^2)
\]

\pause

\vfill %\vspace{0.5cm}

% {\bf This equation is often expressed in matrix notation as:}

% \[
% {\bf y} = {\bf X} {\bm{\beta}} + {\bm \varepsilon}
% \]

% where $\bf X$ is a \alert{design matrix} and $\bm{\beta}$ is a
% vector of coefficients. %\pause More on matrix notation later\dots
Linear models can also be written this way:
\begin{gather*}
  \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} \\
  y_i \sim \mathrm{Normal}(\mu_i, \sigma^2)
\end{gather*}

\end{frame}




\begin{frame}
  \frametitle{Interpreting the $\beta$'s}
You must be able to interpret the $\beta$
coefficients for {\it any model} that you fit to your data.
\pause
\vfill
A linear model might have dozens of continuous and categorical
predictors variables, with dozens of associated $\beta$ coefficients.
\pause
\vfill
%% Key points for interpretting $\beta$'s:
%% \begin{itemize}
%%   \item For continuous explano
%% \end{itemize}
Linear models can also include polynomial terms and interactions
\end{frame}


\begin{frame}[fragile]
  \frametitle{Interpreting the $\beta$'s}
  \small 
  The intercept $\beta_0$ is the expected value of $y$, when all $x=0$ \\
  \pause
  \vfill
  If $x_1$ is a {\bf continuous} explanatory variable: %, $\beta$ is
  \begin{itemize}
    \item $\beta_1$ can usually be interpreted as a \textit{slope}
      parameter
    \item In this case, $\beta_1$ is the
      change in $y$ resulting from a 1 unit change in $x_1$ (while
      holding the other predictors constant)
    \end{itemize}
\pause
\vfill
<<linmod,include=FALSE>>=
set.seed(3400)  
x1 <- runif(100, 0, 50)  
y <- rnorm(100, 10 + 1*x1, 5)
plot(x1, y)
abline(lm(y~x1))
@
\centering
\begin{columns}
  \begin{column}{0.5\textwidth}
<<linmod-out,size='tiny'>>=
lm(y~x1)
@ 
  \end{column}
  \begin{column}{0.4\textwidth}
  \includegraphics[width=\textwidth]{figure/linmod-1} \\
  \end{column}
\end{columns}
\end{frame}




\begin{frame}[fragile]
  \frametitle{\small Interpreting $\beta$'s for categorical explanatory variables}
  Things are more complicated for {\bf categorical} explanatory
  variables (i.e., factors) because they must be converted to dummy
  variables
  \pause
  \vfill
  There are many ways of creating dummy variables
  \pause
  \vfill
%  For a {\bf categorical} explanatory variable %, $\beta$ is
  In \R, the default method for creating dummy variables from
  unordered factors works like
  this: %unordered factors is called \inr{"contr.treatment"}
  \begin{itemize}
    \item One level of the factor is treated as a \alert{reference level}
    \item The reference level is associated with the intercept
    \item The $\beta$ coefficients for the other levels of the factor
      are differences from the reference level
  \end{itemize}
%   \pause
%   \vfill
%   The default method corresponds to:
% <<contr-trt,size='small'>>=
% options(contrasts=c("contr.treatment","contr.poly"))
% @
\end{frame}





\begin{frame}[fragile]
  \frametitle{\small Interpreting $\beta$'s for categorical explanatory variables}
\small 
<<linmod-xc,include=FALSE>>=
set.seed(3400)  
xc <- gl(4, 25) 
y <- rnorm(100, model.matrix(~xc)%*%c(10,1,-1,2), 5)
ym <- tapply(y, xc, mean)
yse <- sqrt(sum(resid(lm(y~xc))^2)/96)/sqrt(25)
bpx <- barplot(ym, ylim=c(0, 15), xlab="Treatment group",
               ylab="Group mean", cex.lab=1.3)
arrows(bpx, ym, bpx, ym+yse, angle=90, code=3, length=0.05)
@
\centering
%\begin{columns}
%  \begin{column}{0.5\textwidth}
<<linmod-xc-out,size='tiny'>>=
lm(y~xc)
@ 
%  \end{column}
%  \begin{column}{0.4\textwidth}
  \includegraphics[width=0.5\textwidth]{figure/linmod-xc-1} \\
%  \end{column}
%\end{columns}
\end{frame}





% \begin{frame}[fragile]
%   \frametitle{\small Interpretting $\beta$'s for categorical explantory variables}
%   Another common method for creating dummy variables results in
%   $\beta$'s that can be interpretted as the $\alpha$'s from the
%   additive models that we saw earlier in the class.
%   \pause
%   \vfill
%   With this method:
%   \begin{itemize}
%     \item The $\beta$ associated with each level of the factor is the
%       difference from the intercept
%     \item The intercept can be interpetted as the grand mean if the
%       continuous variables have been centered
%     \item One of the levels of the factor will not be displayed
%       because it is redundant when the intercept is estimated
%   \end{itemize}
%   \pause
%   \vfill
%   This method corresponds to:
% <<contr-sum,size='small',eval=FALSE>>=
% options(contrasts=c("contr.sum","contr.poly"))
% @
% \end{frame}



% \subsection{Example}



\begin{frame}
  \frametitle{Ruffed grouse data}
  \centering
  \includegraphics[width=\textwidth]{figs/RUGR}
\end{frame}



\begin{frame}
  \frametitle{Ruffed grouse data}
  \centering
  \includegraphics[width=\textwidth]{figs/grouse_map_locs}
\end{frame}




\begin{frame}[fragile]
  \frametitle{Ruffed grouse data}
  \small
  Import the data and convert \inr{route} and \inr{utmZone} to
  factors. 
<<read-grouse,size='tiny'>>=
grouse.data <- read.csv("grouse_data_glm.csv", row.names=1)
grouse.data$route <- factor(grouse.data$route)
grouse.data$utmZone <- factor(grouse.data$utmZone)
str(grouse.data)
@   
\end{frame}



\begin{frame}[fragile]
  \frametitle{Grouse data}
  Fit a linear model to the abundance data
<<grouse-fm1,size='scriptsize'>>=
fm1 <- lm(abundance ~ elevation + utmZone, grouse.data)
summary(fm1)
@   
\end{frame}




\begin{frame}[fragile]
  \frametitle{Grouse data}
  Predict grouse abundance at a sequence of elevations for
  each region (east or west side) of the study area. \\
  \vfill
  First, create a new \inr{data.frame} with the explanatory values of
  interest.
<<grouse-pred-dat,size='scriptsize'>>=
elev.min <- min(grouse.data$elevation)
elev.max <- max(grouse.data$elevation)
seq.length <- 20 ## Determines how smooth the function looks in GLMs 
elev.seq <- seq(from=elev.min, to=elev.max, length.out=seq.length)
pred.data.west <- data.frame(elevation=elev.seq, utmZone="16S")
pred.data.east <- data.frame(elevation=elev.seq, utmZone="17S")
@
\pause
\vfill
  Now get the predictions.
<<grouse-pred,size='scriptsize'>>=
pred.west <- predict(fm1, newdata=pred.data.west, se=TRUE)
pred.east <- predict(fm1, newdata=pred.data.east, se=TRUE)
@ 
\end{frame}



\begin{frame}[fragile]
  \frametitle{Grouse data}
<<grouse-pred-plot,fig.width=7,fig.height=5,out.width="0.85\\textwidth",fig.align='center',size='scriptsize'>>=
plot(abundance ~ elevation, data=grouse.data, ylim=c(0,2))
lines(elev.seq, pred.west$fit, col="blue", lwd=2)
lines(elev.seq, pred.east$fit, col="grey", lwd=2)
legend(900, 2, c("West", "East"), lty=1, col=c("blue","grey"), lwd=2)
@ 
\end{frame}



\section{Generalized linear models}



\begin{frame}[plain]
  \frametitle{Outline}
  \Large
  \tableofcontents[currentsection]
\end{frame}




\begin{frame}
  \frametitle{Generalized linear models (GLMs)}
  \large
  \uncover<1->{{\bf Benefits of generalized linear models}}
  \begin{itemize}%[<+->]
    \item<2-> The residuals don't have to be normally distributed
    \item<3-> The response variable can be binary, integer,
      strictly-positive, etc...
    \item<4-> The variance is not assumed to be constant
    \item<5-> Useful for manipulative experiments or observational
      studies, just like linear models.
  \end{itemize}
  \vfill
  \uncover<6->{
  {\bf Examples}
  \begin{itemize}
    \item Presence-absence studies
    \item Studies of survival
    \item Seed germination studies
  \end{itemize}
  }
\end{frame}



\begin{frame}
  \frametitle{Two important GLMs}
  {\bf Logistic regression \\}
  \begin{itemize}
    \item The response variable is usually binary and modeled with a
      binomial distribution
    \item The probability of success is usually a logit-linear
      model
  \end{itemize}
  \pause
  \vfill
  {\bf Poisson regression \\}
  \begin{itemize}
    \item The response variable is a non-negative integer modeled with
      a Poisson distribution
    \item The expected count is usually modeled with a log-linear
      model
  \end{itemize}
  \vfill
\end{frame}



\begin{frame}
  \frametitle{From linear to generalized linear}
  {\bf Linear model}
  \begin{gather*}
    \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} \\
    y_i \sim \mathrm{Normal}(\mu_i, \sigma^2)
  \end{gather*}
  \pause
  \vfill
  {\bf Generalized Linear model}
  \begin{gather*}
    g(\mu_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} \\
    y_i \sim f(\mu_i)
  \end{gather*}
  \pause
  {\bf where} \\
  $g$ is a link function, such as the log or logit link \\
  \pause
  $f$ is a probability distribution such as the binomial or Poisson
%  that determines (usually) the variance %(there is no $\sigma^2$ parameter!)
\end{frame}


\begin{frame}
  \frametitle{Alternative representations}
  {\bf This:}
  \begin{gather*}
    g(\mu_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} \\
    y_i \sim f(\mu_i)
  \end{gather*}
  \pause
  {\bf Is the same as this:}
  \begin{gather*}
    \mu_i = g^{-1}(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}) \\
    y_i \sim f(\mu_i)
  \end{gather*}
  % \pause
  % {\bf Is the same as this:}
  % \begin{gather*}
  %   g(\mu_i) = {\bf X}{\bm \beta} \\
  %   y_i \sim f(\mu_i)
  % \end{gather*}
\end{frame}


\begin{frame}
  \frametitle{Link functions}
%  \begin{itemize}[<+->]
%    \item
  An inverse link function ($g^{-1}$) transforms values from the $(-\infty,\infty)$
  scale to the scale of interest, such as $(0,1)$ for probabilities  \\
  \pause
  \vfill
%    \item
  The link function ($g$) does the reverse \\
%    \item
%  \pause
%  \vfill
%  The two link functions that you will see most often are the
%      ``logit'' and ``log'' links.
%  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Link functions}
  \centering
  \begin{tabular}{llcc}
    \hline
    Distribution & link name\footnote{\scriptsize These are the most common link functions, but others are available} & link equation             & inverse link equation       \\
    \hline
%    Normal       & identity  & $\mu$                     & ${\bf X}{\bm \beta}$  \\
%                 &           &                           &                             \\
    Binomial     & logit     & $\log(\frac{p}{1-p})$ & $\frac{\exp({\bf
          X}{\bm \beta})}{1 + \exp({\bf X}{\bm \beta})}$                        \\
                 &           &                           &                             \\
    Poisson      & log       & $\log(\lambda)$               & $\exp({\bf X}{\bm \beta})$  \\
    \hline
  \end{tabular}

\pause
\vfill

\begin{tabular}{llcc}
    \hline
    Distribution & link name & link in {\bf R}  & inv link in {\bf R}       \\
    \hline
    Binomial     & logit     & {\tt qlogis} & {\tt plogis}                        \\
                 &           &                           &                             \\
    Poisson      & log       & {\tt log}    & {\tt exp}  \\
    \hline
  \end{tabular}
\end{frame}






\begin{frame}[fragile]
  \frametitle{Logit link example}
  \vspace{-5pt}
  \scriptsize
<<logit-p,size='tiny'>>=
beta0 <- 5
beta1 <- -0.08
elevation <- 100
(logit.p <- beta0 + beta1*elevation)
@
\pause
{How do we convert \Sexpr{logit.p} to a probability? \pause Use the
  inverse-link: \\}
<<inv-logit,size='tiny'>>=
p <- exp(logit.p)/(1+exp(logit.p))
p
@
\pause
{Same as:}
<<plogis,size='tiny'>>=
plogis(logit.p)
@
\pause
{To go back, use the link function itself:}
<<logit,size='tiny'>>=
log(p/(1-p))
qlogis(p)
@
\end{frame}



\begin{frame}[fragile]
  \frametitle{Logit link example}
<<nologit,fig.show='hide',fig.width=6,fig.height=4,size='scriptsize'>>=
plot(function(x) 5 + -0.08*x, from=0, to=100,
     xlab="Elevation", ylab="logit(prob of occurrence)")
@
%\begin{center}
\centering
  \includegraphics[width=\textwidth]{figure/nologit-1} \\
%\end{center}
\end{frame}




\begin{frame}[fragile]
  \frametitle{Logit link example}
<<logit2,fig.show='hide',fig.width=6,fig.height=4,size='scriptsize'>>=
plot(function(x) plogis(5 + -0.08*x), from=0, to=100,
     xlab="Elevation", ylab="Probability of occurrence")
@
%\begin{center}
\centering
  \includegraphics[width=\textwidth]{figure/logit2-1} \\
%\end{center}
\end{frame}





\section{Logistic regression}


\begin{frame}
  \frametitle{Logistic Regression}
%  \begin{itemize}%[<+->]
%    \item<1->
  Logistic regression is a specific type of GLM in which the
      response variable follows a binomial distribution and the link
      function is the logit \\
  \pause
  \vfill
%    \item<2->
  It would be better to call it ``binomial regression'' since other
      link functions (e.g. the probit) can be used \\
%    \item<3->
  \pause
  \vfill
  Appropriate when the response is binary or a count with an
  upper limit
%    \item<4->
  \pause
  \vfill
  {\bf Examples:}
      \begin{itemize}
        \normalsize
        \item Presence/absence studies
        \item Survival studies
        \item Disease prevalence studies
      \end{itemize}
%  \end{itemize}
\end{frame}



%\begin{frame}[fragile]
%  \frametitle{Logistic Regression}
<<simFrogs,echo=FALSE,results='hide'>>=
set.seed(43340)
n <- 30
elev <- round(runif(n, 0, 500))
habitat <- gl(3, 10, labels=c("Oak", "Maple", "Pine"))
beta0 <- -1
beta1 <- 0.01
mu <- plogis(beta0 + beta1*elev)
summary(mu)
y <- rbinom(n, 1, mu)
frogData <- data.frame(presence=y,
                       abundance=rpois(n, exp(beta0 + beta1*elev)),
                       elevation=elev, habitat)
glm1 <- glm(presence ~ elev+habitat, family=binomial(link="logit"), data=frogData)
summary(glm1)
anova(glm1)
@
%\end{frame}



\begin{frame}
  \frametitle{Logistic Regression}
    \begin{gather*}
      \mathrm{logit}(p_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots \\
      y_i \sim \mathrm{Binomial}(N, p_i)
  \end{gather*}
  \pause
  {\bf where: \\}
  $N$ is the number of ``trials'' (e.g. coin flips) \\
  $p_i$ is the probability of success for sample unit $i$
\end{frame}



\begin{frame}[fragile]
  \frametitle{Binomial distribution}% - fair coin}
  \vspace{-0.4cm}
  \note{Have students flip coins}
\begin{center}
<<binom1,echo=FALSE,fig.width=7,fig.height=6,out.width="0.9\\textwidth">>=
plot(0:5, dbinom(0:5, 5, 0.5), type="h",
     xlab="Number of 'successes'", ylab="Probability",
     lend="butt", lwd=5, col="blue", ylim=c(0,0.6),
     main="Binomial(N=5, p=0.5)")
@
\end{center}
\vfill
\end{frame}



\begin{frame}[fragile]
  \frametitle{Binomial distribution}% - warped coin}
  \vspace{-0.4cm}
\begin{center}
<<binom2,echo=FALSE,fig.width=7,fig.height=6,out.width="0.9\\textwidth">>=
plot(0:5, dbinom(0:5, 5, 0.9), type="h",
     xlab="Number of 'successes'", ylab="Probability",
     lend="butt", lwd=5, col="blue", ylim=c(0,0.6),
     main="Binomial(N=5, p=0.9)")
@
\end{center}
\end{frame}




\begin{frame}
  \frametitle{Binomial Distribution}
  {\bf Properties}
  \begin{itemize}
    \item The expected value of $y$ is $Np$
    \item The variance is $Np(1-p)$
  \end{itemize}
  \pause
  \vfill
  {\bf Bernoulli distribution}
  \begin{itemize}
    \item The Bernoulli distribution is a binomial distribution with a
      single trial ($N=1$)
%    \item Think of it as a single coin flip
    \item Logistic regression is usually done in this context, such
      that the response variable is 0/1 or No/Yes or Bad/Good, etc$\dots$
  \end{itemize}
\end{frame}


% \section{The {\tt glm} function}

%\subsubsection{Example}




\begin{frame}
  \frametitle{Logistic regression example}
  \begin{enumerate}
  \item How could we fit the following model to the grouse data?
    \begin{gather*}
      \mathrm{logit}(p_i) = \beta_0 + \beta_1\mathrm{ELEV}_i \\
      y_i \sim \mathrm{Bern}(p_i)
    \end{gather*}
  \item How can we predict and graph occurrence probability ($p$) over a range of
    elevations?
  \end{enumerate}
\end{frame}



\begin{frame}[fragile]
\frametitle{Logistic regression example}
<<logitreg,size='scriptsize'>>=
logitreg1 <- glm(presence ~ elevation, data=grouse.data,
                 family=binomial(link="logit"))
logitreg1
@   
\end{frame}



\begin{frame}[fragile]
  \frametitle{Logistic regression example}
  Predict grouse occurrence probability at a sequence of elevations \\ 
  \vfill
  First, create a new \inr{data.frame} with the explanatory values of
  interest.
<<grouse-lrpred-dat,size='scriptsize'>>=
elev.min <- min(grouse.data$elevation)
elev.max <- max(grouse.data$elevation)
seq.length <- 20 ## Determines how smooth the function looks in GLMs 
elev.seq <- seq(from=elev.min, to=elev.max, length.out=seq.length)
pred.data.lr <- data.frame(elevation=elev.seq)
@
\pause
\vfill
  Now get the predictions.
<<grouse-lrpred,size='scriptsize'>>=
pred.elev <- predict(logitreg1, newdata=pred.data.lr, type="link",
                     se=TRUE)
@ 
\end{frame}



\begin{frame}[fragile]
  \frametitle{Logistic regression example}
  \small
  Predictions and standard error interval. We add $\pm 1$ SE on
    the logit scale, and then transform to the probability scale.
<<grouse-lrpred-plot,fig.width=7,fig.height=5,out.width="0.75\\textwidth",fig.align='center',size='tiny'>>=
plot(presence ~ elevation, data=grouse.data, ylim=c(0,1))
lines(elev.seq, plogis(pred.elev$fit), col="blue", lwd=2)
lines(elev.seq, plogis(pred.elev$fit+pred.elev$se.fit), col="blue", lwd=1, lty=2)
lines(elev.seq, plogis(pred.elev$fit-pred.elev$se.fit), col="blue", lwd=1, lty=2)
@ 
\end{frame}




\section{Poisson regression}



\begin{frame}
  \frametitle{Poisson Regression}
  \Large
    \begin{gather*}
      \mathrm{log}(\lambda_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} \\
      y_i \sim \mathrm{Poisson}(\lambda_i)
  \end{gather*}
  \pause
  {\bf where: \\}
  $\lambda_i$ is the expected value of $y_i$ \\
\end{frame}



\begin{frame}
  \frametitle{Poisson regression}
  \large
  {\bf Useful for:}
  \begin{itemize}
    \item Count data
    \item Number of events in time intervals
    \item Other types of integer data
  \end{itemize}
  \pause
  \vfill
  {\bf Properties}
  \begin{itemize}
    \item The expected value of $y$ ($\lambda$) is equal to the variance
    \item This is an assumption of the Poisson model
    \item Like all assumptions, it can be relaxed if you have enough data
  \end{itemize}
\end{frame}




\begin{frame}[fragile]
  \frametitle{Poisson distribution}
<<pois1,fig.show='hide',echo=FALSE>>=
x <- 0:25
plot(x, dpois(x, lambda=1), type="h", lwd=5, col="blue", lend="butt",
     xlab="Response variable", ylab="Probability",
     main=expression(paste("Poisson(", lambda, "= 1)", sep="")), cex.lab=1.5 )
@
<<pois2,fig.show='hide',echo=FALSE>>=
plot(x, dpois(x, lambda=5), type="h", lwd=5, col="blue", lend="butt",
     xlab="Response variable", ylab="Probability",
     main=expression(paste("Poisson(", lambda, "= 5)", sep="")), cex.lab=1.5 )
@
<<pois3,fig.show='hide',echo=FALSE>>=
plot(x, dpois(x, lambda=10), type="h", lwd=5, col="blue", lend="butt",
     xlab="Response variable", ylab="Probability",
     main=expression(paste("Poisson(", lambda, "= 10)", sep="")), cex.lab=1.5 )
@
\begin{center}
  \only<1>{\includegraphics[width=0.75\textwidth]{figure/pois1-1}}
  \only<2 | handout:0>{\includegraphics[width=0.75\textwidth]{figure/pois2-1}}
  \only<3 | handout:0>{\includegraphics[width=0.75\textwidth]{figure/pois3-1}}
\end{center}
\end{frame}









\begin{frame}[fragile]
  \frametitle{Log link example}
  \footnotesize
<<nolog,fig.show='hide',fig.width=7,fig.height=5,size='footnotesize'>>=
plot(function(x) 5 + -0.08*x, from=0, to=100,
     xlab="Elevation", ylab="log(Expected abundance)")
@
\begin{center}
  \includegraphics[width=0.8\textwidth]{figure/nolog-1}
\end{center}
\end{frame}




\begin{frame}[fragile]
  \frametitle{Log link example}
  \footnotesize
<<log,fig.show='hide',fig.width=7,fig.height=5,size='footnotesize'>>=
plot(function(x) exp(5 + -0.08*x), from=0, to=100,
     xlab="Elevation", ylab="Expected abundance")
@
\begin{center}
  \includegraphics[width=0.8\textwidth]{figure/log-1}
\end{center}
\end{frame}









% \begin{frame}
%   \frametitle{In-class exercise}
%   \begin{enumerate}
%     \item Fit the following model to the grouse data
%       \begin{gather*}
%         \mathrm{log}(\lambda_i) = \beta_0 + \beta_1\mathrm{ELEV}_i + \beta_2\mathrm{Zone17S}\\
%         y_i \sim \mathrm{Pois}(\lambda_i)
%       \end{gather*}
%     \item Predict and graph the expected value of abundance ($\lambda$)
%       over a range of elevations. Use the same sequence of elevations as
%       we used in the previous examples.
%     \item Simulate a new response variable $y$ using the fitted model
%   \end{enumerate}
% \end{frame}


\begin{frame}[fragile]
  \frametitle{Poisson regression simulation}
  \small
  First, create a covariate:
<<pois-cov>>=
n <- 100  
x <- rnorm(n)
@   
  Next, pick values for the coefficients and compute $\lambda_i =
  E(y_i)$:
<<pois-lam>>=
beta0 <- -1
beta1 <- 1
lam <- exp(beta0 + beta1*x)
@   
Now, simulate the response variable $y$:
<<pois-y>>=
y <- rpois(n=n, lambda=lam)
@ 
Finally, fit the model:
<<pois-fit,size='small',eval=FALSE>>=
(poisreg1 <- glm(y ~ x, family=poisson(link="log")))
@ 
\end{frame}



\begin{frame}[fragile]
  \frametitle{Simple (Bayesian) linear regression}
  \small
  The model is in a text file named {\tt glm.jag} \\
<<glm-jag,size="scriptsize",comment="",echo=FALSE>>=
  writeLines(readLines("glm.jag"))
@
\pause
\vfill
Now, put the data in a list and pick some initial values
<<glm-jd>>=
jd.glm <- list(x=x, y=y, n=length(y))
ji.glm <- function() c(beta0=rnorm(1), beta1=rnorm(1))
jp.glm <- c("beta0", "beta1")
@ 
\end{frame}


\begin{frame}[fragile]
  \frametitle{Simple (Bayesian) linear regression}
  Use MCMC to draw posterior samples:
<<glm-jags,size='scriptsize',results='hide',warning=FALSE,cache=FALSE>>=
library(jagsUI)  
js.glm <- jags.basic(data=jd.glm, inits=ji.glm, parameters.to.save=jp.glm,
                     model.file="glm.jag", n.chains=2, n.iter=1000)
@ 
\pause
\begin{columns}
  \begin{column}{0.55\textwidth}
    Summarize the posterior samples:
<<glm-jags-sum,size='scriptsize'>>=
round(summary(js.glm)$quant, 2)
@
\pause
  \end{column}
  \begin{column}{0.45\textwidth}
<<glm-jags-viz,echo=FALSE>>=
plot(js.glm)
@     
  \end{column}
\end{columns}
\end{frame}






\begin{frame}
  \frametitle{Assignment}
  \small
  % \scriptsize
  Create a self-contained R script (or better yet an Rmarkdown file)
  to do the following:
  \begin{enumerate}
    \small
    % \footnotesize
    % \scriptsize
    \item Simulate logistic regression data according to
      $y_i \sim \mathrm{Bern}(p_i)$ and $\mathrm{logit}(p_i) = \beta_0
      + \beta_1 x_i$ with $\beta_0=-1$ and $\beta_1=1$. Generate the
      covariate using \inr{x <- rnorm(100)}.
    \item Fit a logistic regression model to the simulated data ($y$
      and $x$) using the \inr{glm} function. Create a figure showing
      $x$ and $y$ with the fitted regression line. Use
      \inr{predict} to get the fitted line.
    \item Fit the logistic regression model in JAGS. Use the
      $\mathrm{Norm}(0, var=10)$ priors for the regression
      coefficients. Note that, unlike linear regression, there is no
      $\sigma^2$ parameter.
    \item Compare and intepret the estimates of $\beta_0$ and
      $\beta_1$ from the classical analysis and the Bayesian analysis.
    % \item Fit 4 Poisson regression models to the Canada Warbler
    %   data. Try to explain as much variation in abundance as you can
    %   using the two explanatory variables: {\tt elevation} and
    %   {\tt year}. You can use quadratic effects and
    %   interactions. Interpret the estimates from the model with the 
    %   lowest AIC, and create a graph depicting the estimated
    %   relationships. 
  \end{enumerate}
  Upload your {\tt .R} or {\tt .Rmd} file to ELC by 8:00 AM Monday,
  August 30. 
\end{frame}





\end{document}

